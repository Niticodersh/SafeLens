{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qLqIOZxvqc_",
        "outputId": "a230575f-ff85-4208-9811-f01623ebab79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deep_translator import GoogleTranslator\n",
        "import re"
      ],
      "metadata": {
        "id": "bZo4jqkl115t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_hindi(text):\n",
        "    \"\"\"Check if the text contains Hindi characters.\"\"\"\n",
        "    hindi_pattern = re.compile(\"[\\u0900-\\u097F]+\")  # Unicode range for Devanagari script\n",
        "    return bool(hindi_pattern.search(text))"
      ],
      "metadata": {
        "id": "y89SVCnH2ArP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_mixed_text(text, source_lang='hi', target_lang='en'):\n",
        "    \"\"\"\n",
        "    Translate only the Hindi parts of a mixed Hindi-English text to English.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    translated_words = []\n",
        "    for word in words:\n",
        "        if is_hindi(word):  # Translate Hindi words\n",
        "            translated_word = GoogleTranslator(source=source_lang, target=target_lang).translate(word)\n",
        "        else:  # Keep English words unchanged\n",
        "            translated_word = word\n",
        "        translated_words.append(translated_word)\n",
        "    return ' '.join(translated_words)"
      ],
      "metadata": {
        "id": "L41KgALD2CqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def stratified_split(data, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits the dataset into train, validation, and test sets with stratified class distribution,\n",
        "    maintaining 'Category' along with 'text' and 'label'.\n",
        "    \"\"\"\n",
        "    if abs(train_size + valid_size + test_size - 1.0) > 1e-6:\n",
        "        raise ValueError(\"Train, valid, and test sizes must sum to 1.0.\")\n",
        "\n",
        "    texts = [entry['text'] for entry in data]\n",
        "    labels = [entry['label'] for entry in data]\n",
        "    categories = [entry['Category'] for entry in data]\n",
        "\n",
        "    # Combine texts, labels, and categories for stratification\n",
        "    combined_data = list(zip(texts, labels, categories))\n",
        "    combined_texts, combined_labels = zip(*[(text, label) for text, label, _ in combined_data])\n",
        "\n",
        "    # First split into train and temp (valid+test)\n",
        "    train_data, temp_data = train_test_split(\n",
        "        combined_data,\n",
        "        test_size=(valid_size + test_size),\n",
        "        stratify=combined_labels,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Extract texts and labels from temp data for further stratification\n",
        "    temp_texts, temp_labels = zip(*[(text, label) for text, label, _ in temp_data])\n",
        "\n",
        "    # Further split temp data into validation and test\n",
        "    valid_data, test_data = train_test_split(\n",
        "        temp_data,\n",
        "        test_size=test_size / (valid_size + test_size),\n",
        "        stratify=temp_labels,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Format back into dictionaries\n",
        "    train_data = [{'text': text, 'label': label, 'Category': category} for text, label, category in train_data]\n",
        "    valid_data = [{'text': text, 'label': label, 'Category': category} for text, label, category in valid_data]\n",
        "    test_data = [{'text': text, 'label': label, 'Category': category} for text, label, category in test_data]\n",
        "\n",
        "    return train_data, valid_data, test_data\n"
      ],
      "metadata": {
        "id": "KLDnRkfv2II6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distributions(train_data, valid_data, test_data):\n",
        "    \"\"\"Plot class distributions in train, validation, and test sets.\"\"\"\n",
        "    train_labels = [entry['label'] for entry in train_data]\n",
        "    valid_labels = [entry['label'] for entry in valid_data]\n",
        "    test_labels = [entry['label'] for entry in test_data]\n",
        "\n",
        "    # Create subplots\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    datasets = [(\"Train\", train_labels), (\"Validation\", valid_labels), (\"Test\", test_labels)]\n",
        "    for i, (name, labels) in enumerate(datasets, 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        plt.hist(labels, bins=range(3), align='left', rwidth=0.8, color='skyblue', alpha=0.7)\n",
        "        plt.title(f\"{name} Set\")\n",
        "        plt.xlabel(\"Label\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.xticks([0, 1])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OYEtTyY_2LKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(data, filename):\n",
        "    \"\"\"Save the dataset to a CSV file with text and label columns.\"\"\"\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "yMqhuLmc2N0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_excel(input_excel):\n",
        "    \"\"\"\n",
        "    Process the input Excel file to extract 'text' and 'label' columns\n",
        "    and format the data as a list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "        input_excel (str): Path to the input Excel file.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'text' and 'label' keys.\n",
        "    \"\"\"\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(input_excel)\n",
        "\n",
        "    # Extract 'Text' and 'label' columns and format as list of dictionaries\n",
        "    formatted_data = [\n",
        "        {'label': int(row['label']), 'text': row['Text'], 'Category': row['Category']}\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "    return formatted_data"
      ],
      "metadata": {
        "id": "Odb3rIPT2r9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safe_file = '/content/Safe Text.xlsx'\n",
        "unsafe_file = '/content/Unsafe Text.xlsx'\n",
        "\n",
        "safe_df = process_excel(safe_file)\n",
        "unsafe_df = process_excel(unsafe_file)\n",
        "\n",
        "data = safe_df + unsafe_df\n",
        "\n",
        "print(f\"Total records: {len(data)}\")\n",
        "print(\"Sample combined data:\", data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8Hhdjpv1wad",
        "outputId": "7ae67520-e12a-4267-b9fc-ab61f3eb7249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records: 300\n",
            "Sample combined data: [{'label': 0, 'text': 'Wow, this is really inspiring! Your journey is a testament to hard work and perseverance. Thank you for sharing such personal insights with us!', 'Category': 'Optimism'}, {'label': 0, 'text': 'बहुत बढ़िया अंतर्दृष्टि! मैंने आज भौतिकी के बारे में कुछ नया सीखा। यथास्थिति को चुनौती देने वाले नए परिप्रेक्ष्य को देखना हमेशा ताज़ा होता है।', 'Category': 'Learning'}, {'label': 0, 'text': 'This post truly resonated with me. The way you articulate your thoughts makes complex ideas so much more accessible. Keep sharing your wisdom!', 'Category': 'Learning'}, {'label': 0, 'text': 'आपका दृष्टिकोण सचमुच आंखें खोल देने वाला है। ये वार्तालाप होना बहुत महत्वपूर्ण है, और आप इन्हें सुविधाजनक बनाने का उत्कृष्ट कार्य कर रहे हैं।', 'Category': 'Optimism'}, {'label': 0, 'text': 'Such a well-written piece! I appreciate the depth of research you put into this. It shows how passionate you are about your work, and it inspires others to dig deeper too!', 'Category': 'Optimism'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Translate text if it contains Hindi\n",
        "translated_data = []\n",
        "for entry in data:\n",
        "    text = entry['text']\n",
        "    if is_hindi(text):  # Check if text contains Hindi\n",
        "        translated_text = translate_mixed_text(text)\n",
        "    else:\n",
        "        translated_text = text  # Keep English text unchanged\n",
        "    translated_data.append({'text': translated_text, 'label': entry['label'], 'Category': entry['Category']})\n",
        "\n",
        "print(\"Sample translated data:\", translated_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlTq8U3L398G",
        "outputId": "e0218b8d-3679-44dd-8885-310c6791a8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample translated data: [{'text': 'Wow, this is really inspiring! Your journey is a testament to hard work and perseverance. Thank you for sharing such personal insights with us!', 'label': 0, 'Category': 'Optimism'}, {'text': 'Very Excellent insight! I Today Physics Of About In Some? New Learned it. status quo To challenge to give ones New Perspective To Look Always fresh Would Is.', 'label': 0, 'Category': 'Learning'}, {'text': 'This post truly resonated with me. The way you articulate your thoughts makes complex ideas so much more accessible. Keep sharing your wisdom!', 'label': 0, 'Category': 'Learning'}, {'text': 'Yours Approach Really Eyes Shell to give gonna Is. These Conversation Happen Very Important Is, And You These convenient create Of Excellent Work Tax are Are.', 'label': 0, 'Category': 'Optimism'}, {'text': 'Such a well-written piece! I appreciate the depth of research you put into this. It shows how passionate you are about your work, and it inspires others to dig deeper too!', 'label': 0, 'Category': 'Optimism'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Split the dataset into train, validation, and test sets\n",
        "train_data, valid_data, test_data = stratified_split(translated_data)\n",
        "\n",
        "print(f\"Size of Train Set: {len(train_data)}\")\n",
        "print(f\"Size of Validation Set: {len(valid_data)}\")\n",
        "print(f\"Size of Test Set: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiZN9E_b4Iqq",
        "outputId": "b335fcff-8e82-4822-b3f4-337ec57a4c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Train Set: 210\n",
            "Size of Validation Set: 45\n",
            "Size of Test Set: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Save datasets to CSV files\n",
        "save_to_csv(train_data, \"text_train_set.csv\")\n",
        "save_to_csv(valid_data, \"text_valid_set.csv\")\n",
        "save_to_csv(test_data, \"text_test_set.csv\")"
      ],
      "metadata": {
        "id": "DBkI9YmJzV6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}